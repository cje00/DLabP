# -*- coding: utf-8 -*-
"""DA_크롤링_강사작성답안_230615.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GPBlzN4baG0O4bBMQrEmehTTnEqtp4ZP
"""

from bs4 import BeautifulSoup



html_doc = """
<!doctype html>
<html>
    <head>
            <title> 기초 웹 크롤링 </title>
    </head>
    <body>
        크롤링을 해봅시다.
    </body>
</html>
"""

bs_obj = BeautifulSoup(html_doc, "html.parser")
head = bs_obj.find("head")
print(head)

body = bs_obj.find("body")
print(body)

html_doc = """
<!doctype html>
<html>
    <head>
            기초 웹 크롤링 따라하기
    </head>
    <body>
        <div> 첫 번째 부분 </div>
        <div> 두 번째 부분 </div>
    </body>
</html>
"""

bs_obj = BeautifulSoup(html_doc, "html.parser")
body = bs_obj.find("body")
print(body)

div1 = bs_obj.find("div")
print(div1)

div_all = bs_obj.find_all("div")
print(div_all)

div2 = div_all[1]
print(div2)

print(div2.text)

html_doc = """
<!doctype html>
<html>
	<head>
		<title> 기초 웹 크롤링 </title>
	</head>
	<body>
	 <table border = "1">
		<caption> 과일 가격 </caption>
		<tr>
			<th> 상품 </th>
			<th> 가격 </th>
		</tr>
		<tr>
			<td> 오렌지 </td>
			<td> 100 </td>
		</tr>
		<tr>
			<td> 사과 </td>
			<td> 150 </td>
		</tr>
	 </table>

	<table border = "2">
		<caption> 의류 가격 </caption>
		<tr>
			<th> 상품 </th>
			<th> 가격 </th>
		</tr>
		<tr>
			<td> 셔츠 </td>
			<td> 30000 </td>
		</tr>
		<tr>
			<td> 바지 </td>
			<td> 50000 </td>
		</tr>
	 </table>
	</body>
</html>
"""

bs_obj = BeautifulSoup(html_doc, "html.parser")
clothes = bs_obj.find_all("table", {"border":"2"})
print(clothes)

from urllib.request import urlopen
from bs4 import BeautifulSoup

url = "https://ai-dev.tistory.com/1"
html = urlopen(url)
bs_obj = BeautifulSoup(html, "html.parser")
print(bs_obj)

title = bs_obj.find_all("h1")
print(title)

print(title[1].text)

contents = bs_obj.find_all("p")
print(contents)

print(contents[1].text)

from urllib.request import urlopen
from bs4 import BeautifulSoup


url = "https://ai-dev.tistory.com/2"
html = urlopen(url)
# print(html.read())  --> 보기 어려움
bs_obj = BeautifulSoup(html, "html.parser")
print(bs_obj)

table_tag = bs_obj.find_all("table")
table_tag

table_0 = table_tag[0]
table_0



table_tag01  = table_tag[0].find_all("td")
table_tag01

for idx, element in enumerate(table_tag01):
    print(idx, element.text)

table01 = bs_obj.find_all("td")
table01



table = bs_obj.find_all("td", {"style":"width: 33.3333%; text-align: center;"})
table

for idx, element in enumerate(table):
    print(idx, element.text)

com_list = bs_obj.find_all("li")
com_list

com_list01 = bs_obj.find_all("ul", {"style":"list-style-type: disc;"})
com_list01

com_list01 = com_list[0].find_all("li")
for idx, element in enumerate(com_list01):
    print(idx, element.text)

from urllib.request import urlopen
from bs4 import BeautifulSoup

url = "https://www.livesport.com/kr/team/manchester-united/ppjDR086"
html = urlopen(url)
bs_obj01 = BeautifulSoup(html, "html.parser")
win01 = bs_obj.find_all("div",{"class": "formIcon formIcon--w"})
win01

from urllib.request import urlopen
from bs4 import BeautifulSoup
import ssl

ssl._create_default_https_context = ssl._create_unverified_context   #인증서 오류 발생을 피하기 위한 코드
url = "https://www.livesport.com/kr/team/manchester-united/ppjDR086"
html = urlopen(url)
bs_obj01 = BeautifulSoup(html, "html.parser")
win01 = bs_obj.find_all("div",{"class": "formIcon formIcon--w"})
print(bs_obj01)

from selenium import webdriver

driver = webdriver.Chrome('C:/Users/thkim/Downloads/chromedriver_win32')

driver.implicitly_wait(3)
driver.get("https://www.livesport.com/team/manchester-united/ppjDR086")

from bs4 import BeautifulSoup

page = driver.page_source

bs_obj = BeautifulSoup(page, "html.parser")
bs_obj

win = bs_obj.find_all("div", {"class":"formIcon formIcon--w"})
win

draw = bs_obj.find_all("div", {"class":"formIcon formIcon--d"})
draw

lose = bs_obj.find_all("div", {"class": "formIcon formIcon--l"})
lose

n_win = len(win)
n_draw = len(draw)
n_lose = len(lose)
print(n_win)
print(n_draw)
print(n_lose)

report = {'win': n_win, 'draw': n_draw, 'lose':n_lose}
report

max_n = max(report.values())
report.keys()

for key in report:
    if(report[key]==max_n):
        print(key)

from bs4 import BeautifulSoup
from selenium import webdriver

driver = webdriver.Chrome('C:/Users/thkim/Downloads/chromedriver_win32')
driver.implicitly_wait(3)
driver.get("https://www.livesport.com/team/manchester-united/ppjDR086")

page = driver.page_source
bs_obj = BeautifulSoup(page, "html.parser")
win = bs_obj.find_all("div", {"class":"formIcon formIcon--w"})
draw = bs_obj.find_all("div", {"class":"formIcon formIcon--d"})
lose = bs_obj.find_all("div", {"class": "formIcon formIcon--l"})



n_win = len(win)
n_draw = len(draw)
n_lose = len(lose)
report = {'win': n_win, 'draw': n_draw, 'lose':n_lose}
max_n = max(report.values())
for key in report:
    if(report[key]==max_n):
        print(key)

from bs4 import BeautifulSoup
from urllib.request import urlopen
import json as json


endpoint= "https://api.odcloud.kr/api/RealEstateTradingSvc/v1/getRealEstateTradingCount?page=1&perPage=10&cond%5B"
date1 = "RESEARCH_DATE%3A%3ALT%5D=201312&cond%5B"
date2 = "RESEARCH_DATE%3A%3AGT%5D=201301&cond%5B"
region = "REGION_CD%3A%3AEQ%5D=11000&cond%5B"
tradingtype = "DEAL_OBJ%3A%3AEQ%5D=01"
key = "&serviceKey=UYtey0Li1M2psQRIHO1aBUXe%2F0r2GqZCeMVBWk9whx3JEJB7N5rRgHQsgBvWUP7y6%2F3Kk4AhwUuGHHfG6z9QsQ%3D%3D"


url = endpoint+date1+date2+region+tradingtype+key

print(url)

html = urlopen(url)
bs_obj = BeautifulSoup(html, "html.parser")
print(bs_obj)

from bs4 import BeautifulSoup
from urllib.request import urlopen
import pandas as pd
import json as json



endpoint= "https://api.odcloud.kr/api/RealEstateTradingSvc/v1/getRealEstateTradingCount?page=1&perPage=23&cond%5B"
key = "&serviceKey=UYtey0Li1M2psQRIHO1aBUXe%2F0r2GqZCeMVBWk9whx3JEJB7N5rRgHQsgBvWUP7y6%2F3Kk4AhwUuGHHfG6z9QsQ%3D%3D"
date1 = "RESEARCH_DATE%3A%3ALT%5D=202012&cond%5B"
date2 = "RESEARCH_DATE%3A%3AGT%5D=201901&cond%5B"
region = "REGION_CD%3A%3AEQ%5D=11110&cond%5B"
tradingtype = "DEAL_OBJ%3A%3AEQ%5D=05"





url = endpoint+date1+date2+region+tradingtype+key

html = urlopen(url)
bs_obj = BeautifulSoup(html, "html.parser")
print(bs_obj)

data = json.loads(bs_obj.text)
region = data['data'][0]['REGION_NM']
region

date = data['data'][0]['RESEARCH_DATE']
date

count = data['data'][0]['ALL_CNT']
count

row1 = [region, date, count]


row2 = [data['data'][0]['REGION_NM'], data['data'][0]['RESEARCH_DATE'],  data['data'][0]['ALL_CNT']]
print(row1)
row2

df = pd.DataFrame(columns = ['지역', '날짜', '거래 건수'])
df

col = df.columns
print(col)

df_row = pd.Series(row1, index=col)
df_row

df.append(df_row, ignore_index=True)

for i in data['data']:
    row = [i['REGION_NM'], i['RESEARCH_DATE'], int(i['ALL_CNT'])]
    df_row = pd.Series(row, index=col)
    df = df.append(df_row, ignore_index=True)

print(df)

import matplotlib.pyplot as plt                              #1

plt.figure(figsize=(20,10))                                  #2
plt.plot(df['날짜'], df['거래 건수'], color='b', marker='o') #3
plt.show()                                                   #4

from bs4 import BeautifulSoup
from urllib.request import urlopen
import pandas as pd
import matplotlib.pyplot as plt

endpoint= "https://api.odcloud.kr/api/RealEstateTradingSvc/v1/getRealEstateTradingCount?page=1&perPage=23&cond%5B"
key = "&serviceKey=UYtey0Li1M2psQRIHO1aBUXe%2F0r2GqZCeMVBWk9whx3JEJB7N5rRgHQsgBvWUP7y6%2F3Kk4AhwUuGHHfG6z9QsQ%3D%3D"
date1 = "RESEARCH_DATE%3A%3ALT%5D=202012&cond%5B"
date2 = "RESEARCH_DATE%3A%3AGT%5D=201901&cond%5B"
list_region = ["11110", "11215", "11620"]    #종로구, 광진구, 관악구 지역 코드를 리스트로 저장
tradingtype = "DEAL_OBJ%3A%3AEQ%5D=05"

#region = "REGION_CD%3A%3AEQ%5D=11110&cond%5B"

m = len(list_region)                                      #1
df = pd.DataFrame(columns=['지역', '날짜', '거래 건수'])  #2
col = df.columns                                          #3

for i in range(0,m):                                                    #1
    region = f"REGION_CD%3A%3AEQ%5D={list_region[i]}&cond%5B"           #2
    url = endpoint+date1+date2+region+tradingtype+key                  #3
    html = urlopen(url)                                                #4
    bs_obj = BeautifulSoup(html, "html.parser")                        #5
    data = json.loads(bs_obj.text)                                     #6
    for j in data['data']:                                            #7
        row = [j['REGION_NM'], j['RESEARCH_DATE'], int(j['ALL_CNT'])] #8
        df_row = pd.Series(row, index=col)                            #9
        df = df.append(df_row, ignore_index=True)                    #10

df

df01 = df[df['지역']=='종로구']    #1
df02 = df[df['지역']=='광진구']    #2
df03 = df[df['지역']=='관악구']    #3

plt.figure(figsize=(20,10))                              #4
plt.plot(df01['날짜'], df01['거래 건수'], color = 'b',   #5
         marker='o', linestyle='-', label='Jongro')
plt.plot(df02['날짜'], df02['거래 건수'], color = 'r',   #6
         marker='^', linestyle='--', label='Guangjin')
plt.plot(df03['날짜'], df03['거래 건수'], color = 'g',   #7
         marker='s', linestyle='-.', label='Gwanak')


plt.legend(fontsize=20)     #8
plt.show()                  #9

df.to_csv("./trade01.csv")

df.to_csv("./trade02.csv", encoding='CP949')

df.to_csv("./trade03.csv", encoding='CP949', index=False)

data = pd.read_csv("./trade03.csv", encoding='CP949')
data

df.to_excel("./trade04.xlsx", encoding='CP949', index=False)

data = pd.read_excel("./trade04.xlsx", engine='openpyxl')
data

from urllib.request import urlopen
from bs4 import BeautifulSoup

url = "http://comp.fnguide.com/SVO2/ASP/SVD_main.asp?pGB=1&gicode=A005930&cID=&MenuYn=Y&ReportGB=&NewMenuID=11&stkGb=&strResearchYN="
html = urlopen(url)
bs_obj = BeautifulSoup(html, "html.parser")

date1 = bs_obj.find("span", {"class": "date"})
print(date1.text)

date2 = date1.text
date = date2.replace('[','').replace(']','').replace('/','-')
print(date)

corp_name1 = bs_obj.find_all("h1", {"id":"giName"})
print(corp_name1)

corp_name = corp_name1[0].text
print(corp_name)

code1 = bs_obj.find_all("div", {"class":"corp_group1"})
code2 = code1[0].find("h2")
code = code2.text
print(code)

stock_price1 = bs_obj.find("span",{"id":"svdMainChartTxt11"})
print(stock_price1)

stock_price2 = stock_price1.text
stock_price = int(stock_price2.replace(',','').strip())
print(stock_price)

print(type(stock_price))

fgn_own_ratio1 = bs_obj.find("span", {"id":"svdMainChartTxt12"})
print(fgn_own_ratio1)

fgn_own_ratio = float(fgn_own_ratio1.text)
print(fgn_own_ratio)

rel_return1 = bs_obj.find("span", {"id":"svdMainChartTxt13"})
print(rel_return1)

rel_return = float(rel_return1.text)
print(rel_return)

up_list = bs_obj.find("div", {"class": "corp_group2"})
print(up_list)

dd = up_list.find_all("dd")
print(dd)

per = float(dd[1].text)
print(per)

per_12m = float(dd[3].text)
print(per_12m)

per_ind = float(dd[5].text)
print(per_ind)

pbr = float(dd[7].text)
print(pbr)

div_yid1 = dd[9].text
print(div_yid1)

div_yid2 = div_yid1.replace('%', '')
print(div_yid2)

div_yid = float(div_yid2)
print(div_yid)
type(div_yid)

table1 = bs_obj.find("div", {"id": "div1"})
table2 = table1.find_all("td")
print(table2)

volume1 = table2[1].text
volume = int(volume1.replace(',', '').strip())
print(volume)

trans_price1 = table2[3].text
trans_price = int(trans_price1.replace(',','').strip())
print(trans_price)

mk_cpt_pfr1 = table2[6].text
mk_cpt_pfr = int(mk_cpt_pfr1.replace(',', '').strip())
print(mk_cpt_pfr)

mk_cpt_cm1 = table2[8].text
mk_cpt_cm = int(mk_cpt_cm1.replace(',', '').strip())
print(mk_cpt_cm)

res = [date, corp_name, code, stock_price, fgn_own_ratio, rel_return, per, per_12m, per_ind, pbr, div_yid, volume, trans_price,
      mk_cpt_pfr, mk_cpt_cm]
print(res)

import pymysql

conn = pymysql.connect(host='localhost', user='root', password='1234', db='stock', charset='utf8')

sql_state = """INSERT INTO stock.daily_market(dt, item_name, item_code, price, foreign_ownership_ratio,
            rel_return, per, per_12m, per_ind, pbr, dividend_yield, volume, trans_price, market_capital_prefer,
            market_capital_common) VALUES ('%s', '%s', '%s', %d, %f, %f, %f, %f, %f, %f, %f, %d, %d, %d, %d)"""%(tuple(res))
print(sql_state)

db = conn.cursor()     #1
db.execute(sql_state)  #2
conn.commit()          #3
conn.close()           #4

# Commented out IPython magic to ensure Python compatibility.
from urllib.request import urlopen
from bs4 import BeautifulSoup
import pymysql
import time


def stock_crawling(item):
    url = "http://comp.fnguide.com/SVO2/ASP/SVD_main.asp?pGB=1&gicode=A"+item+"&cID=&MenuYn=Y&ReportGB=&NewMenuID=11&stkGb=&strResearchYN="
    html = urlopen(url)
    bs_obj = BeautifulSoup(html,"html.parser")

    # 날짜
    date1 = bs_obj.find("span", {"class": "date"})
    date2=date1.text
    date = date2.replace('[','').replace(']','').replace('/','-')
    print(date)


    # 기업 정보
    corp_name1 = bs_obj.find_all("h1", {"id":"giName"})
    corp_name = corp_name1[0].text

    # 종목 코드
    code1 = bs_obj.find_all("div", {"class":"corp_group1"})
    code2 = code1[0].find("h2")
    code = code2.text


    # 주가
    stock_price1 = bs_obj.find("span",{"id":"svdMainChartTxt11"})
    stock_price2 = stock_price1.text
    stock_price = int(stock_price2.replace(',','').strip())

    # 외국인 보유 비중
    fgn_own_ratio1 = bs_obj.find("span", {"id":"svdMainChartTxt12"})
    fgn_own_ratio = float(fgn_own_ratio1.text)

    # 상대수익률
    rel_return1 = bs_obj.find("span", {"id":"svdMainChartTxt13"})
    rel_return = float(rel_return1.text)


    # 상단 테이블
    up_list = bs_obj.find("div", {"class": "corp_group2"})
    dd = up_list.find_all("dd")


    # PER
    per = float(dd[1].text)

    # 12M PER
    per_12m = float(dd[3].text)

    # 업종 PER
    per_ind = float(dd[5].text)

    # PBR
    pbr = float(dd[7].text)

    # 배당 수익률
    div_yid1 = dd[9].text
    div_yid2 = div_yid1.replace('%', '')
    div_yid = float(div_yid2)

    # 시세현황 테이블
    table1 = bs_obj.find("div", {"id": "div1"})
    table2 = table1.find_all("td")

    # 거래량
    volume1 = table2[1].text
    volume = int(volume1.replace(',', '').strip())

    # 거래대금
    trans_price1 = table2[3].text
    trans_price = int(trans_price1.replace(',','').strip())

    # 시가총액(우선주 포함)
    mk_cpt_pfr1 = table2[6].text
    mk_cpt_pfr = int(mk_cpt_pfr1.replace(',', '').strip())

    # 시가총액(보통주)
    mk_cpt_cm1 = table2[8].text
    mk_cpt_cm = int(mk_cpt_cm1.replace(',', '').strip())


    # 결과 모음 리스트
    # [날짜, 기업정보, 종목코드, 주가 외국인 보유비중, 상대수익률,]
    # per, 12m per, 업종 per, pbr, 배당수익률
    # 테이블, 거래량, 거래대금, 시가총액(우선주포함), 시가총액(보통주)]

    res = [date, corp_name, code, stock_price, fgn_own_ratio, rel_return, per,
           per_12m, per_ind, pbr, div_yid, volume, trans_price, mk_cpt_pfr, mk_cpt_cm]

    return res



def db_insert(res):
    try:
        conn = pymysql.connect(host='localhost', user='root',
                               password='1234', db='stock',
                               charset='utf8')
        sql_state = """INSERT INTO stock.daily_market(dt, item_name, item_code, price,
                    foreign_ownership_ratio, rel_return, per, per_12m, per_ind, pbr,
                    dividend_yield, volume, trans_price, market_capital_prefer,
                    market_capital_common) VALUES ('%s', '%s', '%s', %d, %f, %f, %f,
#                     %f, %f, %f, %f, %d, %d, %d, %d)"""%(tuple(res))
        db = conn.cursor()
        db.execute(sql_state)
        conn.commit()
    except:
        token = "xoxb-5403354296533-5403427924053-hJgaemuPcIP0LfugVvhSFhS8"
        channel = "#stock_alarm01"
        text = "Check your stock crawler."

    finally:
        conn.close()

if __name__ == '__main__':
    item_list = ['005930', '066570']

    for item in item_list:
        res = stock_crawling(item)
        db_insert(res)
        time.sleep(3)

import requests

token = "xoxb-5403354296533-5403427924053-hJgaemuPcIP0LfugVvhSFhS8"
channel = "#stock_alarm01"
text = "Check your stock crawler."

requests.post("https://slack.com/api/chat.postMessage",
             headers = {"Authorization": "Bearer "+token},
             data={"channel": channel, "text": text})

import pymysql

conn = pymysql.connect(host='localhost', user='root',
                      password='1234', db='stock', charset='utf8')

sql_state = """SELECT * FROM stock.daily_market WHERE dt BETWEEN
                '2023-06-09' AND '2023-06-13';"""

db = conn.cursor()     #1 커서를 생성합니다
db.execute(sql_state)  #2 앞에서 저장한 쿼리 문을 실행합니다
rows = db.fetchall()   #3 fetchall 메서드를 사용해 실행 결과 데이터를 모두 가져옵니다
conn.close()           #4 MySQL과의 연결을 종료합니다

print(rows)

import pandas as pd

colnames = ['seq', 'dt', 'item_name', 'item_code', 'price',
           'foreign_ownership_ratio', 'rel_return', 'per',
           'per_12m', 'per_ind', 'pbr', 'dividend_yield', 'volume',
           'trans_price', 'market_capital_prefer', 'market_capital_common']
df = pd.DataFrame(rows, columns=colnames)
df

df_sam = df[df['item_name']=='삼성전자']
df_lg = df[df['item_name']=='LG전자']

df_sam

df_lg

import matplotlib.pyplot as plt

plt.figure(figsize=(20,20))

plt.subplot(5,2,1)
plt.plot(df_sam['dt'], df_sam['price'], color='blue',marker='o',
        linestyle='-')
plt.title('price')
plt.xticks(rotation=45)

plt.subplot(5,2,2)
plt.plot(df_sam['dt'], df_sam['foreign_ownership_ratio'], color='red',marker='o',
        linestyle='-')
plt.title('foreign_ownership_ratio')
plt.xticks(rotation=45)

plt.subplot(5,2,3)
plt.plot(df_sam['dt'], df_sam['rel_return'], color='brown',marker='o',
        linestyle='-')
plt.title('rel_return')
plt.xticks(rotation=45)

plt.subplot(5,2,4)
plt.plot(df_sam['dt'], df_sam['per'], color='orange',marker='o',
        linestyle='-')
plt.title('per')
plt.xticks(rotation=45)

plt.subplot(5,2,5)
plt.plot(df_sam['dt'], df_sam['pbr'], color='green',marker='o',
        linestyle='-')
plt.title('pbr')
plt.xticks(rotation=45)

plt.subplot(5,2,6)
plt.plot(df_sam['dt'], df_sam['dividend_yield'], color='purple',marker='o',
        linestyle='-')
plt.title('dividend_yield')
plt.xticks(rotation=45)

plt.subplot(5,2,7)
plt.plot(df_sam['dt'], df_sam['volume'], color='gray',marker='o',
        linestyle='-')
plt.title('volume')
plt.xticks(rotation=45)

plt.subplot(5,2,8)
plt.plot(df_sam['dt'], df_sam['trans_price'], color='pink',marker='o',
        linestyle='-')
plt.title('trans_price')
plt.xticks(rotation=45)

plt.subplot(5,2,9)
plt.plot(df_sam['dt'], df_sam['market_capital_prefer'], color='olive',marker='o',
        linestyle='-')
plt.title('market_capital_prefer')
plt.xticks(rotation=45)

plt.subplot(5,2,10)
plt.plot(df_sam['dt'], df_sam['market_capital_common'], color='cyan',marker='o',
        linestyle='-')
plt.title('market_capital_common')
plt.xticks(rotation=45)


plt.subplots_adjust(hspace=0.7)
plt.show()



